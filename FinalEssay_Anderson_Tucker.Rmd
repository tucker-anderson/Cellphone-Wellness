---
title: "W241 Final Project Essay"
author: "Tucker Anderson and Carlos Sancini"
output:
  rmarkdown::github_document
---
## Cell Phone Mindfulness Study

```{r echo=FALSE, results='hide', warning=FALSE, message=FALSE}
library(data.table)
library(stargazer)
library(pwr)
library(ggmap)
```

### Background
Cell phone, or smart phone, usage has increased dramatically in the past few years. Ownership has become so pervasive that now 96% of Americans own a smartphone(https://www.pewinternet.org/fact-sheet/mobile). This permutation of technology has had a tremendous impact the world over. From browsing the internet to texting your friends to playing Candy Crush, it's easier than ever to pull your phone out of your pocket to respond to an email or to try and quash that split-second feeling of boredom. Software is eating the world and the mobile platform is providing users a limitless means to disconnect. This increased connectivity to the web has some concerned that we may be spending too much of our time and attention in the screens of our phones and less time paying attention to the world around us. 

The Screen Time application developed by Apple is included by default in all iOS 12 and greater phones. This application automatically tracks the time and application use of the user, and seemed to be a great way to measure our primary dependent variable of interest: time spent using the phone each week.

Using this application, we seek to determine the effect of mindfulness of phone use on the amount of time spent on a phone during the week.

### Experimental Design
We designed a set of surveys to extract a sample of the population's use of cell phones. We isolated a subset of the population, only reaching out to users who owned an Apple smartphone which had iOS 12 or higher installed, to utilize the Screen Time app and ensure our measurement methodology was consistent between all subjects. We believed the consistent measurement benefit would outweigh the effects of leaving out large subsets of the population such as Android phone users. The assumption here is that there is no fundamental difference at the population scale between Apple smartphone users and users of other smartphone platforms. While it seems possible that there may be some difference between these populations, for the sake of this study we assumed there to be no difference between iOS and other smartphone users.

We ran a quick power analysis to determine the required amount of subjects in the control and treatment group. In this test we assumed a minimum power of 0.80, as per Cohen's reasoning that studies should have a maximum Type II error rate of 0.20, and a significance level of 0.05 as per the standard practice of setting alpha Type I error rates to 0.05. We worried that the effect size of "mindfulness" of screen time could be very small.
```{r}
# determine sample size required to achieve a power of 80% (Type II error rate of 20%) and a Type I error rate of 5% with a negligible effect size
pwr.t.test(d = 0.3, sig.level = 0.05, power = 0.80)
```
We see based on our power calculation that, assuming our effect size would be about 0.3, we would like to achieve a sample size of about 175 for each group, or about 350 total subjects with a perfect 50/50 split. However, our team foresaw a large attrition bias from subjects in this study, it's always hard to get the same subjects to respond to a similar survey after one and especially two weeks. Considering this, with an expectation of a possible 25% attriton rate between weeks, we would like to generate survey responses from 437 participants, rounding up to a clean 500. Accounting for attrition in this way will hopefully keep our statistical power through the weeks, but it still leaves the possible bias of those attriting due to some inexplicable difference between the rest of subjects. Perhaps those who attrit naturally use their phone less or are more perceptible to our treatment. This is something to keep in mind for our analysis.

Our design also had some potential compliance issues; would our subjects properly read the articles given to them? Would treatment and control subjects properly read their articles? We wanted to use Qualtrics' timing feature and link clicking verification to ensure compliance to treatment and control groups were being met. If the subjects did not take a reasonable amount of time to read the article we could remove them from the analysis or weight their results in the final effect analysis.

###Experiment Design Overview
Recruiting:
●	Use of Amazon Mechanical Turk to recruit iPhone owners in USA for a total of 150-200 subjects
●	0.25 will be paid for each report sent (total of 3 reports - US$ 0.75 per subject)
●	Money will be sent just at the end of the experiment for the subjects that complied
●	Subsjects will answer a quick survey and turn on ScreenTime app during recrutiment
●	Subjects will be asked to send screenshots that will be used to certify that the app is turned on
●	Survey information would be used to block and assign subjects to treatment and control

Outcome Measures:
●	After recrutiment we would use a professional online marketing campaign tool (Sender) to control sequenced treatment messages, reminders and instruction on how subjects should proceed during the experiment. The tool can control whether subjects opened the email and clicked the article links  
●	All measurements made throughout iPhone ScreenTime app: total screen time (per day and weekly total) and share of categories (productivity, games, social, reading & reference)
●	Measure of totals for the last seven days (keep subjects from wiping or altering data or at least we will be aware, given that total time should increase after 7 days)
●	Subjects will take screenshots and send them by email


Pre-Treatment:
●	Both control and treatment groups will be intimately aware of being in an experiment. To reduce observer effect, the enrollment message will be focused on a  “new cell phone design study”, and not how much people use it 
●	Visual guides of how to send the report will be made available to subjects.
●	Measures would be collected after one week total recruitment 

Treatment Group:
●	The day after pre-treatment measures, they will receive an email with a link to an article related to digital wellbeing. 
●	The links will be shortened using Google URL Shortener and will be coded to track whether subjects are complying to treatment (to be defined).
●	As an additional compliance effort, they will be asked to respond to the email with a simple agree/don’t agree about the article’s claim
●	After one week of pre-treament measures they will be asked to send ScreenTime screenshots
●	Reminders will be sent in the day before and in  the due date
●	70% of subjects would be assigned to treatment (expected small CACE and probably high variance)
●	In the following week, everything is repeated but with a different article or a video of a testimony of a good experience of digital wellbeing.

Control Group:
●	The same schedule of intervention will take place, however subjects will receive a placebo article links (eg.: comparison between prices and features of different phone brands)
●	Email messages of both treatment and control groups will not have any content that could anticipate the article topic, so that differential compliance is avoided

Covariates:
●	Variables that are predictive of cell phone usage (questionnaire applied in Amazon Mechanical Turk during recruitment): age, gender, phone used for work (yes or no), phone provided by company (yes or no), employed (unemployed,  full time job or part time job), type of employment (white or blue collar), the number of notifications received (ScreenTime statistic), data/time of screen shot report (email received)

Blocking:
●	Knowledge of Screen Time (already activated vs first time activation)
●	White vs Blue collar jobs

### Issues Conducting Experiment
Unfortunately, we were not able to successfully conduct our study as specified above. Amazon's Mechanical Turk suspended our academic and personal accounts for PII violations (but as of 8/13/19 still have not provided what part of the survey questions were deemed to be in violation) and non-American account statuses. Attempting to reconsolidate these issues took weeks off of our schedule and eventually only allowed us to collect partial baseline data three times. In light of this, no true treatment (re-surveying after patients read an article about the benefits of mindfulness of cell phone use) was administered and no effect from our original design can be extracted.

### EDA
Let's quickly explore our baseline data.
```{r echo=FALSE}
# ingest data into a data table
d <- fread('./data/phone_data_location.csv')

d.size <- dim(d)
```
We ended up with `r d.size[1]` baseline records with a number of useful covariates such as gender, blue collar vs white collar, and our dependent variable of interest screen time from the app.  
```{r echo=FALSE, message=FALSE}
# based on geo information, it looks like we have 6 possible duplicate entries
dups <- d[duplicated(d)]
# let's remove them for the sake of possible biases
d <- d[!duplicated(d)]
# looks like theres two records with more than a possible number of minutes per week
lim.min <- dim(d[d$minutes.week > 10080])[1]
# lets remove them
d <- d[d$minutes.week <= 10080]
```
We have `r dups` duplicate values in the dataset and `r lim.min` values that are too high for the maximum minute value of 10080. 

```{r}
hist(d$age)
hist(d$duration)
```

### Data Analysis
```{r}
head(d)
```
```{r}
model.age <- lm(minutes.week ~ age, d)
plot(d$age, d$minutes.week, main = "Cell Phone User Age vs Minutes per Week", 
     xlab = "Age (Years)", ylab = "Minutes of Screentime (per Week)")
abline(model.age)
stargazer(model.age, type = 'text')
```

```{r}
d[, is.whitecollar := employment.type=="White Collar"]
d[, is.bluecollar := employment.type=="Blue Collar"]
d[, is.unemployed := employment.type=="Unemployed"]
model.employment <- lm(minutes.week ~ is.whitecollar + is.unemployed, d)
stargazer(model.employment, type = 'text')
```

### Conclusion
Despite not being able to conduct a true field experiment, we were able to gather a large baseline dataset of iOS users with their screentime and a number of useful covariates. We were not able to conclusively state that mindfulness of phone use reduced screen use, but we were able to identify key covariates that were correlated to increased phone usage. We cannot conclusively find causation, as we made no random interventation! However, the correlations we found could point to further study designs down the road to help us seeking causation.